---
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
```

# Lab 7: Assessing Goodness of Fit Without Hypothesis Testing

## Analyzing the Packet Per Second Data

For assignment 2 you collected number of packets sent per second by one of the CADE machines. While in the assignment you will collect much more data and analyze seconds per packet, for this lab, we'll use a much smaller version of the packet data you will collect, and we'll analyze packets per second.

In the data folder of this lab, you will find `packet_data.csv`. For one of the CADE Machines, this is the cumulative total number of packets sent with a timestamp. While you collect data 5 times per second, here data is collected 3 times per second. The first thing we'll need to do is convert each row to be a number of packets per second.

Here is some code that does that:

```{r}
packetData = read.csv("data/packet_data.csv",header = TRUE) %>%
  mutate(time = as.integer(time)) %>%
  group_by(time) %>%
  summarise(packets = max(total_packets) - min(total_packets))
```

Now we want to know whether this data comes from a poisson distribution. Here are some questions we might ask ourselves to begin to probe this further:

### Question 1: What are a collection of sufficient conditions that would result in the packet data being poisson distributed? [See Question 1 in Canvas] 

While a thorough analysis would discuss the validity of the sufficient conditions in detail, we'll assess the validity of the Poisson distribution to the data solely based off of comparison of probability mass functions.

The strategy is as follows:

We assume the packet arrival data is $X_1,X_2,\dots,X_N$ ($X_i$ being the $i^{th}$ packet arrival, $N = 180$) and these packet arrivals are independent and identically distributed according to $P_D$, where $P_D$ is the probability mass function. So for example $P_D(2) = P(X_1 = 2)$

We'd like to compare $P_D$ with a poisson distribution. 

There are two challenges here


1. We don't have access to $P_D$, so we first need to estimate it with our data
2. There are infinitely many poisson distributions. Which Poisson distribution do we compare our estimate of $P_D$ to?

The law of large numbers (and extensions of it) will help us with both of these questions

For the first challenge, we need to estimate $P_D(x)$ for every $x \in \{0,1,2,3,\dots\}$. For a single $x$, let $\mathbb{I}(X_i = x)$ be $1$ if $X_i = x$ and $0$ otherwise. Then $E(\mathbb{I}(X_i = x)) = P_D(x)$ for each $1 \leq i \leq N$. So versions of the law of large numbers (about which you could learn more if you take a more advanced probability course), give that

\[
\frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(X_i = x) = \frac{\text{Total Number of } X_i= x}{N}
\]
is a good estimator for $P_D(x)$ for large values of $N$. 

**The indicator function is weird, but the average written above is just the proportion of data points equal to $x$**


We'll assume $N$ is large enough for now -- (in a more advanced probability course you would learn about ways to probabalistically bound the deviation between this average and $P_D(x)$ by choosing a sample size large enough)

### Question 2: Using the method just described, what would our estimate be for $P_D(2)$?

Using this method, here is some code that estimates $P_D(x)$ for every $x$ value that occurs in the data.

```{r}
N = nrow(packetData)
meanEst = mean(packetData$packets)
maxObs = max(packetData$packets)
vizPacketData <- packetData %>%
  group_by(packets) %>%
  summarise(p = n()/N)
```

For the non-occurring $x$ values in $\{0,1,2,\dots\}$, the estimate is $0$. So now we have estimates for $P_D(x)$ for each $x \in \{0,1,2,\dots\}$.

[Side note: Because $P_D$ takes on infinitely many values, there is some refinement of the vanilla law of large numbers needed to guarantee that we are uniformily estimating $P_D$ well. But there is a such an argument (the main ideas of it can be seen for example in the Glinvenko-Cantelli theorem) that justifies our method for estimating $P_D$.]

Now that we have an estimate for $P_D$, what is the correct poisson distribution to compare it to?

If $P_D$ is $Poisson(\lambda)$, then by the law of large numbers, $\frac{1}{N} \sum_{i=1}^{N} X_i$ will be a good estimator for $\lambda$ as $N$ gets large. So lets compare our estimate of $P_D$ to a $Poisson(\frac{1}{N} \sum_{i=1}^{N} X_i)$ 

<!-- ### Question 5: As described above, based on the law of large numbers, compute the value of $\lambda$ we will use when we compare the Poisson distribution with our estimate of $P_D$ -->

Here is some code that compares our estimate of $P_D$ with the $Poisson(\frac{1}{N} \sum_{i=1}^{N} X_i)$

```{r,fig.width = 15}
vizPacketData = vizPacketData %>%
   rbind(data.frame(packets = setdiff(0:maxObs,unique(packetData$packets)),
                   p = rep(0,length(setdiff(0:maxObs,unique(packetData$packets)))))) %>%
  mutate(empirical = rep("Estimated P_D(x)",length(p))) %>%
  rbind(data.frame(packets = 0:maxObs,
                   p = dpois(0:maxObs,lambda = meanEst),
                   empirical = rep("Poisson(avg)(x)")))
ggplot(data = vizPacketData,aes(x = packets,y=p,fill = empirical))+geom_bar(position="dodge", stat="identity")+
  scale_x_continuous(breaks=seq(0,80,by=1))
```

The 2 clear outliers in the packet arrival data will have a strong influence on $\frac{1}{N}\sum_{i=1}^{N} X_i$, thereby "pushing" the poisson distribution that we are comparing to farther to the right.

While we recognize that the outliers are a part of the distribution of the packet arrival data, it is also worthwhile to remove them to understand how much of an effect they have on the theoeretical distribution we are comparing to. 

### Question 3: Remove the two outliers, and compute and report the sample mean again.


With the outliers removed, here is the same analysis repeated:

```{r,echo = FALSE}
packetData = packetData %>%
  filter(packets <= 30)
N = nrow(packetData)
meanEst = mean(packetData$packets)
maxObs = max(packetData$packets)
vizPacketData <- packetData %>%
  group_by(packets) %>%
  summarise(p = n()/N)

vizPacketData = vizPacketData %>%
   rbind(data.frame(packets = setdiff(0:maxObs,unique(packetData$packets)),
                   p = rep(0,length(setdiff(0:maxObs,unique(packetData$packets)))))) %>%
  mutate(empirical = rep("Estimated P_D(x)",length(p))) %>%
  rbind(data.frame(packets = 0:maxObs,
                   p = dpois(0:maxObs,lambda = meanEst),
                   empirical = rep("Poisson(avg)(x)")))
ggplot(data = vizPacketData,aes(x = packets,y=p,fill = empirical))+geom_bar(position="dodge", stat="identity")+
  scale_x_continuous(breaks=seq(0,80,by=1))
```

Still not a great fit, but arguably better than before.

<!-- ### Question 6: At what $x$ value do we see the biggest deviation from poissonality? -->
The probability mass function analysis we just did is one way to assess the goodness of fit of the poisson distribution to the data. Question 4 asks you to come up with another:

### Question 4: The Poisson distribution has a special property that $E(Poisson(\lambda)) = Var(Poisson(\lambda)) = \lambda$. Based on this property, devise a metric that if very large, should inform us of a lack of poissonality in the sample

In Question 4, you may be wondering how large is large enough to reject the conclusion of poissonality of the sample. As you will learn later in the course, hypothesis testing and distribution theory provide a framework for determining a threshold for rejecting poissonality of the sample while simultaneously ensuring that we reject the hypothesis of poissonality in the data when the data is actually drawn from the poisson distribution with a fixed probability. But for now, it is enough to know that such technology exists to determine the threshold.

## Analyzing the Ping Data

You collected ping data for assignment 2. While in the assignment you will collect much more data, for this lab, we'll use a much smaller version of the ping data you collected.

In the data folder of this lab, you will find `ping_data.csv`. For one of the CADE Machines, this is the amount of time it takes for a ping to make a round trip. First we'll do some quick preprocessing to remove the $ms$ from the ping column.

```{r}
pingData = read.csv("data/ping_data.csv",header = TRUE) %>%
  mutate(ping = as.numeric(str_split(ping,pattern = " ",simplify = TRUE)[,1]))
```

The ping data is an amount of elapsed time. This is continuous data.

Here is a look at the absolute frequency histogram

```{r,message = FALSE}
ggplot(data = pingData,aes(x=ping))+geom_histogram()+ggtitle("ping time in ms")+xlab("time in ms")+ylab("Count")
```

### Question 5: The data almost looks bell curved. Why technically is the normal distribution an inappropriate fit for this data?

Let $Y_1,Y_2,\dots,Y_N \overset{iid}{\sim} P_D$ where $Y_i$ is the $i^{th}$ ping time in milliseconds. $(N = 538)$ in this data. We want to know $P_D$, where $P_D$ is the induced probability measure of $Y_i,1 \leq i \leq N$. From question 5, we know that technically $P_D$ is not normal.

The exponential distribution is one model that is sufficent when we are measuring the elapsed amount of time between events that occur independently of one another and at a constant average rate. The exponential distribution may actually be **in**appropriate for the ping data. 

The pings are not all sent out at the same time, and we are not measuring the time between ping arrivals on the local machine. If this were the case, we would have a poisson process that would justify usage of the exponential distribution.

Instead, we are sending pings out at different times and simply measuring how long it took them to return. So the time between arrival paradigm is not really satisfied.

Throughout the remainder of the lab, we'll try to fit an exponential distribution to this data anyway. This will motivate alternative strategies you can take on the assignment.

For this lab,lets  attempt to compare $P_D$ to the $exp(\lambda)$. 

There are again two steps to this process:

1. We don't have access to $P_D$, so we have to estimate it using the data $Y_1,\dots,Y_N$
2. There are an infinite number of exponential distributions. So again, how do choose which one to compare to?

The second question is more straightforward, so lets answer that one first:

Note that if $Y \sim exp(\lambda)$, then
\[
  \lambda = 1/E(Y)
\]

### Question 6: Using the law of large numbers as your motivation, if we suppose $P_D = exp(\lambda)$, then what do you think would be a good estimate for $\lambda$ based on the sample?
 
Answering question $1$ is a little more nuanced because we are dealing with continuous data. 

But as discussed in lecture, the idea is that the histogram for the data is a reasonable approximation for the density $f_d$ provided the bins are not too large and the sample size is large enough.

Make a histogram for the data with the $exponential(1/[\frac{1}{N}\sum_{i=1}^{N} Y_i])$ density overlayed on it.

Some code that might be helpful is the following [You may need to play around with things in the console to get things working

```{r,eval = FALSE}
h = hist(pingData$ping,freq=FALSE,xlim = c(0,max(pingData$ping)))
```


```{r,eval = FALSE}
rateEst = 1/mean(pingData$ping)
xlines = seq(0,max(h$breaks),length.out = 100)
lines(x = xlines,y=dexp(xlines,rate = rateEst))

```
If you are having trouble, you can always clear the canvas and start over by running `dev.off()` in the console.

### Question 7: Based on the plot you made (with the exponential density function over), do you think the exponential distribution is a good model for this data?

There are other ways besides density analysis to compare the distribution of the data to the theoretical distribution of interest.For example, we could have attempted to compare cdfs instead. 

Lets go in this direction. Let $F$ denote the CDF of the iid sample $Y_1,\dots,Y_N$.

We will compare $F$ to the cdf of the $exponential(1/(\frac{1}{N}\sum_{i=1}^{N} Y_i))$ distribution.

### Question 8: Use the law of large numbers to estimate $F(.15) = P(Y_1 \leq .15)$. Then use the `pexp` function to find $P(exp(1/mean(pingData)) <= .15)$. Compute the absolute difference.

[Hint: the law of large number justifies the use of proportions to estimate probabilities for iid data. If this is not helpful, look back at the method described just before question 2. If that is not helpful, ask us]

We can generalize what we did for $.15$ to perform a more complete analysis.

Just to be more explicit, if we denote $\hat{F}$ as the empirical cdf for our sample $Y_1,Y_2,\dots,Y_N$, then for $x \in \mathbb{R}$
\[
\hat{F}(x) = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(Y_i \leq x) = \frac{\text{Total Number of } Y_i \leq x}{N}
\]

Your answer to question $8$ is $\hat{F}(.15)$. And there is a famous theorem in probability (Glinvenko-Cantelli) that says $\hat{F}$ is in a sense a uniformily good approximation of $F$ as $N$ gets large. We'll assume $N$ is large enough. 

Lets now compare $\hat{F}$ to the cdf of the $exponential(1/mean(pingData))$

Some code to do this is displayed below.

```{r}
P = ecdf(pingData$ping)
x = seq(from = 0,to = 1,by = .01)
empirical = P(x)
target = pexp(x,rate = 1/(mean(pingData$ping)))
Fun = c("Empirical CDF","Exp(1/mean(ping)) CDF")
visualFrame = data.frame(x = rep(x,times = 2),
                         y = c(empirical,target),
                         Fun = rep(Fun,each=length(x)))
ggplot(data = visualFrame,aes(x=x,y=y,color = Fun))+geom_line()+xlab("x")+ylab("CDF value")

```

When you look at this plot, questions to ask yourself are:

- In what range of the data is the exponential cdf most inappropriate?
- Are there any areas of the data where the exponential cdf is actually not so bad?

To be more quantitative in our assessment, there is a measure called the Kolmogorov-Smirnov statistic, that finds the "maximum" distance between the empirical cdf and the candidate cdf. (Those of you who like real analysis may note that defining "maximum" distance in this setting involves some care, but we will ignore this detail in this course)

### Question 9: On the grid used in the code, find and report the maximum absolute difference between the empirical cdf and the candidate cdf

We discussed that the normal distribution is technically not an appropriate model for the ping data. But that doesn't necessarily mean that in practice it is a bad fit -- especially if the mean of the elapsed times is large and the variance is very small.

If you are working on the assignment, here are two things you could try

- Plot the cdf of the Normal distribution (with mean and standard deviation estimated using the data) against the emprical cdf in the same way that was done here
- The gamma distribution is a more flexible extension of the exponential distribution that doesn't allow for negative values, but allows for a symmetric distribution with appropriately selected parameters. Maybe it could provide a good fit in this setting. 




