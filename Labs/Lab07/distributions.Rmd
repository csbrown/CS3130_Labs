---
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
```

# Lab 7: Visually exploring Goodness of Fit

## Intro

This lab is related to Assignment 2.  In assignment 2, you are gathering data from the CADE machines with the intent to gain insights into the population of "CADE machine states at times when nobody is using them".  Keep this in mind throughout the lab.

## Analyzing the Packet Data

For assignment 2 you collected packet data. While in the assignment you will collect much more data and analyze seconds per packet, for this lab, we'll use a much smaller version of the packet data.  Specifically, we'll be looking at the number of IP packets received by the machine over time.

In the data folder of this lab, you will find `packet_data.csv`. For one of the CADE Machines, this is the cumulative total number of packets sent with a timestamp. While you collect data 5 times per second, here data is collected 3 times per second. The first thing we'll need to do is convert each row to be a number of packets per second.

Here is some code that does that:

```{r}
packetData = read.csv("data/packet_data.csv",header = TRUE) %>%
  mutate(time = as.integer(time)) %>%
  group_by(time) %>%
  summarise(packets = max(total_packets) - min(total_packets))
```

We want to know whether this data comes from a Poisson distribution. Here are some questions we might ask ourselves to begin to probe this further.

Recall the assumptions that we made about a process generating our data when we derived the Poisson distribution in class.  [A Poisson Process](https://en.wikipedia.org/wiki/Poisson_point_process#Complete_independence) requires that the number of occurrences in an interval be *independent* of the interval and independent of the number of occurrences in previous intervals.

### Question 1: Based on your knowledge of how the internet works, do you think that the arrivals of IP packets meet these conditions?  Might they approximately meet these conditions?  Explain.

It is often the case that our a priori assumptions about things such as independence might well be wrong.  We can investigate the situation further by comparing this data with the theoretical Poisson distribution to the data solely based off of comparison of probability mass functions.  If the data are not generated by a Poisson process, it is unlikely that a Poisson distribution will be a very good fit.  If the data ARE generated by a Poisson process, then for large sample sizes the Poisson distribution should be a good fit.

So, if our a priori thoughts were that the assumptions for a Poisson process were met by our IP packet process, and we subsequently notice a good fit, this might give us confidence in our a priori reasoning.  If we subsequently notice a poor fit, this might cause us to revisit our assumptions.

OTOH, if our a priori thoughts were that the assumptions for a Poisson process were NOT met by our IP packets process, and we subsequently notice a good fit, this might suggest that we should revisit our assumptions.  If we subsequently notice a poor fit, this might give us confidence in our a priori reasoning.

So, let $P_D$ be the actual distribution of the number of IP packets that arrive per unit time.  We'd like to compare $P_D$ with a Poisson distribution. 

There are two challenges here

1. We don't have access to $P_D$, so we first need to estimate it with our data
2. There are infinitely many Poisson distributions. Which Poisson distribution do we compare our estimate of $P_D$ to?

The law of large numbers (and extensions of it) will help us with both of these questions

For our data, recall that in order to make any kind of good inferences, you need for your sample to be *representative*.  One way to get a decently representative sample is to take a *simple random sample* so that each data point in your *population* is equally likely.

### Question 2: What is the population we are trying to study here?  If we gather data for an hour, is this a simple random sample from this population?  Might you still expect this sample to be representative?  Explain.

For the first challenge, we need to estimate $P_D(x)$ for every $x \in \{0,1,2,3,\dots\}$. For a single $x$, we can estimate the probability of getting that number of IP packet arrivals as the relative frequency of getting that number of IP packet arrivals in our dataset.  If our data are representative, we expect this to be a pretty good estimate.  If our data are simple random, we expect this estimate to be good for a "large" sample size.

Let's assume that our data are representative for now.

### Question 3: Using the method just described, what would our estimate be for $P_D(2)$?

Using this method, here is some code that estimates $P_D(x)$ for every $x$ value that occurs in the data.

```{r}
N = nrow(packetData)
meanEst = mean(packetData$packets)
maxObs = max(packetData$packets)
vizPacketData <- packetData %>%
  group_by(packets) %>%
  summarise(p = n()/N)
#print(vizPacketData)
```

For the non-occurring $x$ values in $\{0,1,2,\dots\}$, the estimate is $0$. So now we have estimates for $P_D(x)$ for each $x \in \{0,1,2,\dots\}$.

[Side note: Because $P_D$ takes on infinitely many values, there is some refinement of the vanilla law of large numbers needed to guarantee that we are uniformily estimating $P_D$ well. But there is a such an argument (the main ideas of it can be seen for example in the [Glinvenko-Cantelli theorem](https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem)) that helps to justify our method for estimating $P_D$.] ()

Now we have an estimate for $P_D$, the distribution of our IP packet arrival times. 

What is an appropriate Poisson distribution to compare it to?  Again, note that there are infinitely many such distributions!  However, there is usually one that is the most likely distribution given that data that we have.  This is called the ["Maximum Likelihood Estimate"](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).  Fortunately, in many common scenarios, the Maximum Likelihood Estimate for a thing is identical to the sample point estimate for a value.  So, we just need to use our data to estimate the Poisson distribution parameter $\lambda$, the average rate of occurrences per unit time.  If a Poisson distribution is going to fit our data, this one will be the most likely.

We can obtain such a point estimate as $\frac{1}{N} \sum_{i=1}^{N} X_i$.

Here is some code that compares our estimate of $P_D$ against $\text{Poisson}(\frac{1}{N} \sum_{i=1}^{N} X_i)$

```{r,fig.width = 15}
vizPacketData = vizPacketData %>%
   rbind(data.frame(packets = setdiff(0:maxObs,unique(packetData$packets)),
                   p = rep(0,length(setdiff(0:maxObs,unique(packetData$packets)))))) %>%
  mutate(empirical = rep("Estimated P_D(x)",length(p))) %>%
  rbind(data.frame(packets = 0:maxObs,
                   p = dpois(0:maxObs,lambda = meanEst),
                   empirical = rep("Poisson(avg)(x)")))
ggplot(data = vizPacketData,aes(x = packets,y=p,fill = empirical))+geom_bar(position="dodge", stat="identity")+
  scale_x_continuous(breaks=seq(0,80,by=1))
```

The 2 clear outliers in the packet arrival data will have a strong influence on $\frac{1}{N}\sum_{i=1}^{N} X_i$, thereby "pushing" the poisson distribution that we are comparing to farther to the right.

While we recognize that the outliers are a part of the distribution of the packet arrival data, it is also worthwhile to remove them to understand how much of an effect they have on the theoeretical distribution we are comparing to.

### Question 4: Remove the two outliers, and compute and report the sample mean again.

With the outliers removed, here is the same analysis repeated:

```{r,echo = FALSE}
packetData = packetData %>%
  filter(packets <= 30)
N = nrow(packetData)
meanEst = mean(packetData$packets)
maxObs = max(packetData$packets)
vizPacketData <- packetData %>%
  group_by(packets) %>%
  summarise(p = n()/N)

vizPacketData = vizPacketData %>%
   rbind(data.frame(packets = setdiff(0:maxObs,unique(packetData$packets)),
                   p = rep(0,length(setdiff(0:maxObs,unique(packetData$packets)))))) %>%
  mutate(empirical = rep("Estimated P_D(x)",length(p))) %>%
  rbind(data.frame(packets = 0:maxObs,
                   p = dpois(0:maxObs,lambda = meanEst),
                   empirical = rep("Poisson(avg)(x)")))
ggplot(data = vizPacketData,aes(x = packets,y=p,fill = empirical))+geom_bar(position="dodge", stat="identity")+
  scale_x_continuous(breaks=seq(0,80,by=1))
```

Arguably better than before, but still not a great fit.  Since we can't even blame the poor fit on outliers, it's not really worth trying to see if they were anomalous.  Overall, this seems to provide some indication that perhaps IP packet arrival times are not generated by a Poisson process.

The probability mass function analysis we just did is one way to assess the goodness of fit of the poisson distribution to the data. Question 5 asks you to come up with another:

### Question 5: The Poisson distribution has a special property that $E(Poisson(\lambda)) = Var(Poisson(\lambda)) = \lambda$. Based on this property, devise a metric that if very large, should inform us of a lack of Poissonality in the sample

In question 5, you may be wondering "how large is large enough to reject the conclusion of Poissonality of the sample?".  Similarly to how the protagonists of past labs have measured evidence, one can couch this in terms of probabilities.  "Suppose that our population *were* Poisson... what is the probability of getting a simple random sample with percent difference in the expected value and variance at least $x$?".  We won't go through this process here, but this thought process of devising a metric of deviation and then computing the probability of large deviation of *a sample* from some hypothesis about *the population* will be a recurring theme in the course.

## Analyzing the Ping Data

You collected ping data for assignment 2. While in the assignment you will collect much more data, for this lab, we'll use a much smaller version of the ping data you collected.

In the data folder of this lab, you will find `ping_data.csv`. For one of the CADE Machines, this is the amount of time it takes for a ping to make a round trip. First we'll do some quick preprocessing to remove the $ms$ from the ping column.

```{r}
pingData = read.csv("data/ping_data.csv",header = TRUE) %>%
  mutate(ping = as.numeric(str_split(ping,pattern = " ",simplify = TRUE)[,1]))
```

The ping data is an amount of elapsed time. This is continuous data.

Here is a look at the absolute frequency histogram

```{r,message = FALSE}
ggplot(data = pingData,aes(x=ping))+geom_histogram()+ggtitle("ping time in ms")+xlab("time in ms")+ylab("Count")
```

For question 6, recall the support of the normal distribution.  Also, consider carefully the nature of the population and variable under study.

### Question 6: The data almost looks bell curved.  What property of the variable under study might give us pause in attempting to model this with a Normal distribution?

Compute and interpret `pnorm(0, mean=10, sd=1)`.  Based on this, sometimes the property that we've identified in q6 does not have enough of an effect on our problem to have to resort to more complicated things, such as the [truncated normal](https://en.wikipedia.org/wiki/Truncated_normal_distribution).  As a general rule, real world data will *never* be *perfectly* modeled by the theoretical distributions we are studying.  Often, however, these models are close enough to be *useful*.  A [famous quote by statistician George Box](https://en.wikipedia.org/wiki/All_models_are_wrong) paraphrases this.

Find estimates of the mean and standard deviation of the ping data.  Use these to plot the most likely theoretical Normal distribution from which our data might be arising given our data, if our data source is actually Normal.  Plot this against the density distribution of your sample.

### Question 7: What is the probability of getting a negative value in the theoretical Normal distribution for which you've estimated the parameters?


<!--
The exponential distribution is one model that is sufficent when we are measuring the elapsed amount of time between events that occur independently of one another and at a constant average rate. The exponential distribution may actually be **in**appropriate for the ping data. 

The pings are not all sent out at the same time, and we are not measuring the time between ping arrivals on the local machine. If this were the case, we would have a poisson process that would justify usage of the exponential distribution.

Instead, we are sending pings out at different times and simply measuring how long it took them to return. So the time between arrival paradigm is not really satisfied.

Throughout the remainder of the lab, we'll try to fit an exponential distribution to this data anyway. This will motivate alternative strategies you can take on the assignment.

For this lab,lets  attempt to compare $P_D$ to the $exp(\lambda)$. 

There are again two steps to this process:

1. We don't have access to $P_D$, so we have to estimate it using the data $Y_1,\dots,Y_N$
2. There are an infinite number of exponential distributions. So again, how do choose which one to compare to?

The second question is more straightforward, so lets answer that one first:

Note that if $Y \sim exp(\lambda)$, then
\[
  \lambda = 1/E(Y)
\]

### Question 6: Using the law of large numbers as your motivation, if we suppose $P_D = exp(\lambda)$, then what do you think would be a good estimate for $\lambda$ based on the sample?
 
Answering question $1$ is a little more nuanced because we are dealing with continuous data. 

But as discussed in lecture, the idea is that the histogram for the data is a reasonable approximation for the density $f_d$ provided the bins are not too large and the sample size is large enough.

Make a histogram for the data with the $exponential(1/[\frac{1}{N}\sum_{i=1}^{N} Y_i])$ density overlayed on it.

Some code that might be helpful is the following [You may need to play around with things in the console to get things working

```{r,eval = FALSE}
h = hist(pingData$ping,freq=FALSE,xlim = c(0,max(pingData$ping)))
```


```{r,eval = FALSE}
rateEst = 1/mean(pingData$ping)
xlines = seq(0,max(h$breaks),length.out = 100)
lines(x = xlines,y=dexp(xlines,rate = rateEst))

```
If you are having trouble, you can always clear the canvas and start over by running `dev.off()` in the console.

### Question 7: Based on the plot you made (with the exponential density function over), do you think the exponential distribution is a good model for this data?

There are other ways besides density analysis to compare the distribution of the data to the theoretical distribution of interest.For example, we could have attempted to compare cdfs instead. 

Lets go in this direction. Let $F$ denote the CDF of the iid sample $Y_1,\dots,Y_N$.

We will compare $F$ to the cdf of the $exponential(1/(\frac{1}{N}\sum_{i=1}^{N} Y_i))$ distribution.

### Question 8: Use the law of large numbers to estimate $F(.15) = P(Y_1 \leq .15)$. Then use the `pexp` function to find $P(exp(1/mean(pingData)) <= .15)$. Compute the absolute difference.

[Hint: the law of large number justifies the use of proportions to estimate probabilities for iid data. If this is not helpful, look back at the method described just before question 2. If that is not helpful, ask us]

We can generalize what we did for $.15$ to perform a more complete analysis.

Just to be more explicit, if we denote $\hat{F}$ as the empirical cdf for our sample $Y_1,Y_2,\dots,Y_N$, then for $x \in \mathbb{R}$
\[
\hat{F}(x) = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(Y_i \leq x) = \frac{\text{Total Number of } Y_i \leq x}{N}
\]

Your answer to question $8$ is $\hat{F}(.15)$. And there is a famous theorem in probability (Glinvenko-Cantelli) that says $\hat{F}$ is in a sense a uniformily good approximation of $F$ as $N$ gets large. We'll assume $N$ is large enough. 

Lets now compare $\hat{F}$ to the cdf of the $exponential(1/mean(pingData))$

Some code to do this is displayed below.

```{r}
P = ecdf(pingData$ping)
x = seq(from = 0,to = 1,by = .01)
empirical = P(x)
target = pexp(x,rate = 1/(mean(pingData$ping)))
Fun = c("Empirical CDF","Exp(1/mean(ping)) CDF")
visualFrame = data.frame(x = rep(x,times = 2),
                         y = c(empirical,target),
                         Fun = rep(Fun,each=length(x)))
ggplot(data = visualFrame,aes(x=x,y=y,color = Fun))+geom_line()+xlab("x")+ylab("CDF value")

```

When you look at this plot, questions to ask yourself are:

- In what range of the data is the exponential cdf most inappropriate?
- Are there any areas of the data where the exponential cdf is actually not so bad?

To be more quantitative in our assessment, there is a measure called the Kolmogorov-Smirnov statistic, that finds the "maximum" distance between the empirical cdf and the candidate cdf. (Those of you who like real analysis may note that defining "maximum" distance in this setting involves some care, but we will ignore this detail in this course)

### Question 9: On the grid used in the code, find and report the maximum absolute difference between the empirical cdf and the candidate cdf


We discussed that the normal distribution is technically not an appropriate model for the ping data. But that doesn't necessarily mean that in practice it is a bad fit -- especially if the mean of the elapsed times is large and the variance is very small.

If you are working on the assignment, here are two things you could try

- Plot the cdf of the Normal distribution (with mean and standard deviation estimated using the data) against the emprical cdf in the same way that was done here
- The gamma distribution is a more flexible extension of the exponential distribution that doesn't allow for negative values, but allows for a symmetric distribution with appropriately selected parameters. Maybe it could provide a good fit in this setting. 
-->



