---
title: 'Lab 9: Confidence Intervals For the Mean'
output: html_document
---

We'll be analyzing how Americans spend their time.

The [American Time Use Survey](https://www.bls.gov/tus/overview.htm#1) is an annual survey that provides nationally representative data about the amount of time Americans spend doing various activities, such as commuting to work, watching television, or caring for children, and provides ancillary variables such as demographic characteristics and other factors.  These data are frequently used by researchers, journalists, educators, sociologists, economists, government policy-makers, lawyers, city planners and individuals such as yourself.

The 2020 survey interviewed $8,782$ people.  Just a handful of variables collected include:
 
 <ul>
  <li>TRTFRIEND: Total nonwork-related time respondent spent with friends (in minutes) [Min:0, Max: 1440] </li>
  <li>TRTSPONLY: Total nonwork-related time respondent spent with spouse only (in minutes) </li>
  <li>TRTCCC: Total nonwork-related time respondent spent with customers, clients, and coworkers (in minutes)</li>
  <li>TRTALONE: Total nonwork-related time respondent spent alone (in minutes) </li>
  <li>TRTFAMILY: Total nonwork-related time respondent spent with family members 
 </li>
 <li>TRCHILDNUM: Number of household Children < 18 </li>
 <li> TRERNWA: Weekly earnings at main job [For non-hourly wage earners]</li>
 <li> TRERNHLY: Hourly earnings at main job [For hourly wage earners]</li>
 <li> TUCC2: Time first child < 13 woke up [For those with children -- in number of minutes past midnight]</li>
 <li> TUCC4: Time last household child < 13 went to bed [For those with children] -- in number of minutes past midnight</li>
</ul>

The population of interest is Americans and how they spend their time on a typical weekday (in 2020). So there are $10$ distributions of interest, and we would like to do inference for the mean of each of these distributions.

Although contrived, from now on we will assume that the entire ATUS survey results **are the entire** population -- i.e we are assuming that every American is included in the ATUS survey results.  The point of this is so that we can investigate the relationship between "confidence", a sample, and the population.  In practice, of course, if one were attempting to make inferences about the time use of Americans, one would simply use the entire ATUS dataset. 

i.e will use a random subsampling of the ATUS survey results to do inference about the population of all Americans surveyed by ATUS.

You can find most of this "population" data in `data/populationData/nonNAdata.csv`. After loading in this dataset, answer question 1.

## Question 1: Use the "population" dataset to compute the mean total time spent alone (in minutes) on a week day for Americans in 2020

Using an approach like that you used in question 1, here are our population means for the other variables.

*  TRTFRIEND: 22.228 minutes 
*  TRTSPONLY: 114.421 minutes
*  TRTCCC: 3.964 minutes
*  TRTFAMILY: 295.0263 minutes
*  TRCHILDNUM: .6522 children 
*  TRERNWA: 1176.6790\$
*  TRERNHLY: 21.3226\$
*  TUCC2: 451.7254 minutes
*  TUCC4: 1212.3868 minutes


## Question 2: What is the "population" standard deviation for time spent alone (the standard deviation of the entire ATUS data)?

As a general rule, you don't have access to the population.  We will attempt to make inferences about the population using a *sample*.  Let's simulate taking a sample of 200 independent and identically distribution observations for each variable.

Let $\bar{X}_{200}$ be the sample mean time spent alone. Because our samples are independent and identically distributed from the population, the CLT gives 
\[
\frac{\sqrt{n}}{\sigma} (\bar{X}_n - \mu) \overset{d}{\to} N(0,1)
\]

where $\sigma$ is the population standard deviation time spent alone (in minutes) and $\mu$ is the population mean time spent alone (in minutes).

There are a couple of details we are glossing over here. One is that we don't necessarily know that the variance of the distribution of time spent alone even exists. And even if it did, we learned last week that the [skewness of a distribution determines the speed of convergence in the CLT](https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem).  If you like, you can employ the techniques from Lab 8 to estimate the sampling distribution of the mean for $n=200$, and thereby the speed of convergence.

But for now, lets assume that the variance of the distribution of time spent alone does exist, and higher moments of this distribution behave in such a way that
$\bar{X}_{200} \overset{d}{\approx} N(\mu,\sigma^2/200)$ is not a bad approximation.

Side note: If you compare the probability density of an appropriately paramaterized exponential distribution with the histogram of time spent alone, you can see a decent fit. So based on our investigation both in the last several lectures and in last weeks lab, $n = 200$ should be more than enough for the CLT approximation to be a good one.

The standard deviation of the entire TRTALONE variable is about $290m$.

We can approximate a 90% two-sided confidence interval using our Normal approximation as:

```{r eval=F}
c(qnorm(alpha/2, mean=your_sample_mean, sd=290/sqrt(n)), qnorm(1-alpha/2, mean=your_sample_mean, sd=290/sqrt(n)))
```

This is equivalent to:

$\bar{x} \pm$ `qnorm(1-alpha/2, mean=0, sd=1) * 290/sqrt(n)`

Which is similar to the $t$-distribution calculation we learned in class.

*Draw a picture of this with pen-and-paper, if you have it*.  Figure out what alpha should be, and compute the above confidence interval.

## Question 3: Using the CLT, and assuming for a moment that the population standard deviation is $290$ minutes, and assuming a sample of size $n=200$, use `qnorm` to use a Normal approximation to estimate the length of a $90\%$ confidence interval for the population mean time spent alone be?

Usually we don't have access to the population standard deviation, so we estimate it using the sample data.

As we've discussed, the law of large numbers provides justification for using the sample standard deviation $\hat{\sigma}_n$ to estimate the population standard deviation.

Also, the [algebra of convergence in distribution](https://en.wikipedia.org/wiki/Slutsky%27s_theorem) justifies replacing $\sigma$ with $\hat{\sigma}_n$ in statements about convergence in distribution.

So it is valid to use the sample standard deviation ($\hat{\sigma}_n$) in place of the population standard deviation when applying the CLT. Specifically, instead of using the approximation
\[
\bar{X}_{200} \overset{d}{\approx} N(\mu,\sigma^2/200)
\]
we can use the approximation
\[
\bar{X}_{200} \overset{d}{\approx} N(\mu,\hat{\sigma}_{200}^2/200)
\]

In class, we learned that the exact distribution of our *t-statistic*: $\frac{\bar{X}_{200} - \mu}{\hat{\sigma}_{200}/\sqrt{n}}$ is the *t-distribution* -- **But only when the population is normally distributed**

In a sense, using the $t$-distribution instead of the Normal distribution to compute confidence intervals for the mean accounts for *some* of the approximation error due to using the sample standard deviation - but only as much as can be accounted for with Normally-distributed populations.  The remaining "extra" error depends on "unknown unknowns" for which we simply have to lean on our convergence to Normal result (i.e. taking a large sample) in an unquantifiable way.

Side note: If you *happen to know the exact distribution of a statistic*, you should use it (rather than an approximation like the CLT) to construct the confidence interval.  We will see situations like this in future.

Since the $t$-statistic tends toward a Normal distribution, an immediate corrollary is that as the degrees of freedom of the $t$-*distribution* tends to infinity, the $t$-distribution converges to the standard normal. i.e 
\[t_{df} \overset{d}{\to} N(0,1) \text{ as }df \to \infty\]

Since the $t$-distribution accounts for *some* of the additional error due to using the sample standard deviation, then in general it is better to use the $t$-distribution to estimate your confidence interval rather than the Normal distribution, *regardless* of whether the population is normally distributed.  In the extremely unlikely event that your population has an "even more central" distribution than Normal because the population under study is miraculously well-behaved, then using the $t$-distribution will give overly-conservative estimates of confidence (i.e. you should actually feel *more* confident than the $t$-distribution suggests).  In the likely event that the population has a "less central" distribution than Normal, then using the $t$-distribution will give under-conservative estimates of confidence, but less under-conservative than the Normal distribution approximation.

However, since the $t$-statistic (and distribution) tend toward Normal, then the difference between:
\[
\frac{\sqrt{200}}{\hat{\sigma_n}}(\bar{X}_{200}-\mu) \overset{\approx}{\sim} t_{199}
\]

and

\[
  \frac{\sqrt{n}}{{\hat{\sigma}_{200}}} (\bar{X}_{200}  - \mu) \overset{\approx}{\sim} N(0,1)
\]

Will be very small.

Since both of these distribution are symmetric, in $90\%$ confidence interval construction, we are interested in the $95^{th}$ quantile of whatever distribution we believe our statistic follows. The $95^{th}$ quantile of the $t_{199}$ is $1.652547$ while the $95^{th}$ quantile of $N(0,1)$ is $1.644854$. Not much difference.

In other words, when $n$ is large, both of these approaches will lead to very similar inferences. The probability that a $t$ based interval will cover the mean is slightly higher than that of the $N(0,1)$ (CLT) based approach since the $t$ distribution has heavier tails than the $N(0,1)$.

In the rest of the lab, we will independently collect a sample of 200 indiduals from each of the 10 ATUS variables and then construct exact $90\%$ confidence intervals for the mean for each variable.

Let $Y =$ Number of Intervals Not Covering the Mean

## Question 4: What is the distribution of $Y$?

## Question 5: What is $P(Y \geq 1)$?. Hint: $1 - P(Y = 0)$. Note: Even if you aren't sure about Question 4, you can still answer this question using properties of independent events

Here is some code that does this procedure using our Normal approximation.  If you are interested, you can compare this with the $t$-distribution, and also with an estimate of the actual sampling distribution, as we've done in class.  You'll fill in a couple of details to make the code work. Then you'll construct the 10 intervals using the filled in code.  The key here is the `?sample` function.

```{r,eval = FALSE}
#Read in the data
dataSubset1 <- read.csv("data/populationData/nonNAdata.csv",header = TRUE)
m <- nrow(dataSubset1)
n <- 200
sampledOne <- dataSubset1[sample(m,size = n,replace = TRUE),]
trernhlyS <- sample(read.csv("data/populationData/TRERNHLYadjusted.csv")[,2],size = n,replace = TRUE)
trernwaS <- sample(read.csv("data/populationData/TRERNWAadjusted.csv")[,2],size = n,replace = TRUE)
tucc2S <- sample(read.csv("data/populationData/TUCC2adjusted.csv")[,2],size = n,
                 replace = TRUE)
tucc4S <- sample(read.csv("data/populationData/TUCC4adjusted.csv")[,2],size = n,
                 replace = TRUE)
sampledData <- cbind(sampledOne,trernhlyS,trernwaS,tucc2S,tucc4S)[,-1]

#get the sample means
mns <- apply(X = sampledData, MARGIN = 2, FUN = mean)
#get the sample sds
sds <- apply(X = sampledData, MARGIN = 2, FUN = sd)

#Get the quantile needed for CI construction
qntl <- ? [see question 7]

#construct the confidence intervals 
intervalLow <- mns - qntl*(sds/sqrt(n))
intervalHigh <- mns + qntl*(sds/sqrt(n))

#These are the population means.
trueMeans <- c(22.228,114.421,3.964,362.0627,295.0263,.6522,21.3226,1176.6790,451.7254,1212.3868)

frame <- data.frame(intervalLow,trueMeans,intervalHigh)

numFailedIntervals <- ? [see question 8]
```

## Question 6: If we use the Normal approximation, what code should be filled in on the line where `qntl` is defined in order to construct the $90\%$ (CLT based) confidence intervals?

Now based on your answer to question $6$, fill in the line for `qntl` and run the code.

## Question 7: Report the *actual* number of intervals that failed to cover the population mean.

Go around the class and ask your peers what *they* got for Question 7.  Compare this with your results from question 5.

## Question 8: If instead we decided to make intervals based on the $t$ distribution -- still $90\%$ -- what code should be filled in on the line where `qntl` is defined?

Compute a few of these and compare them to your Normal distribution estimate.  Which are more "conservative", the Normal distribution CIs or the $t$-distribution CIs?
