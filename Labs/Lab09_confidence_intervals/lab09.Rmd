---
title: 'Lab 9: Confidence Intervals For the Mean'
output: html_document
---

We'll be analyzing how Americans spend their time.

The [American Time Use Survey](https://www.bls.gov/tus/overview.htm#1) is an annual survey that provides nationally representative data about the amount of time Americans spend doing various activities, such as commuting to work, watching television, or caring for children, and provides ancillary variables such as demographic characteristics and other factors.  These data a frequently used by researchers, journalists, educators, sociologists, economists, government lawmakers, lawyers, city planners and individuals such as yourself.

The 2020 survey interviewed $8,782$ people.  Some variables collected include:
 
 <ul>
  <li>TRTFRIEND: Total nonwork-related time respondent spent with friends (in minutes) [Min:0, Max: 1440] </li>
  <li>TRTSPONLY: Total nonwork-related time respondent spent with spouse only (in minutes) </li>
  <li>TRTCCC: Total nonwork-related time respondent spent with customers, clients, and coworkers (in minutes)</li>
  <li>TRTALONE: Total nonwork-related time respondent spent alone (in minutes) </li>
  <li>TRTFAMILY: Total nonwork-related time respondent spent with family members 
 </li>
 <li>TRCHILDNUM: Number of household Children < 18 </li>
 <li> TRERNWA: Weekly earnings at main job [For non-hourly wage earners]</li>
 <li> TRERNHLY: Hourly earnings at main job [For hourly wage earners]</li>
 <li> TUCC2: Time first child < 13 woke up [For those with children -- in number of minutes past midnight]</li>
 <li> TUCC4: Time last household child < 13 went to bed [For those with children] -- in number of minutes past midnight</li>
</ul>

The population of interest is Americans and how they spend their time on a typical weekday (in 2020). So there are $10$ distributions of interest, and we would like to do inference for the mean of each of these distributions.

Although contrived, from now on we will assume that the entire ATUS survey results **are the entire** population -- i.e we are assuming that every American is included in the ATUS survey results.  The point of this is so that we can investigate the relationship between "confidence", a sample, and the population.  In practice, of course, if one were attempting to make inferences about the time use of Americans, one would simply use the entire ATUS dataset. 

i.e will use a random subsampling of the ATUS survey results to do inference about the population of all Americans surveyed by ATUS.

You can find most of this "population" data in `data/populationData/nonNAdata.csv`. After loading in this dataset, answer question 1.

## Question 1: Use the population dataset to compute the mean total time spent alone (in minutes) on a week day for Americans in 2020

Using an approach like that you used in question 1, here are our population means for the other variables.

*  TRTFRIEND: 22.228 minutes 
*  TRTSPONLY: 114.421 minutes
*  TRTCCC: 3.964 minutes
*  TRTFAMILY: 295.0263 minutes
*  TRCHILDNUM: .6522 children 
*  TRERNWA: 1176.6790\$
*  TRERNHLY: 21.3226\$
*  TUCC2: 451.7254 minutes
*  TUCC4: 1212.3868 minutes


## Question 2: What is the population standard deviation for time spent alone?

Now lets imagine we didn't have access to the population. We only have access to a sample of 200 independent and identically distribution observations for each variable.

Let $\bar{X}_{200}$ be the sample mean time spent alone. Because our samples are independent and identically distributed from the population, the CLT gives 
\[
\frac{\sqrt{n}}{\sigma} (\bar{X}_n - \mu) \overset{d}{\to} N(0,1)
\]

where $\sigma$ is the population standard deviation time spent alone (in minutes) and $\mu$ is the population mean time spent alone (in minutes).

There are a couple of details we are glossing over here. One is that we don't necessarily know that the variance of the distribution of time spent alone even exists. And even if it did, we learned last week that the [skewness of a distribution determines the speed of convergence in the CLT](https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem).

But for now, lets assume that the variance of the distribution of time spent alone does exist, and higher moments of this distribution behave in such a way that
$\bar{X}_{200} \overset{d}{\approx} N(\mu,\sigma^2/200)$ is not a bad approximation.

Side note: If you compare the probability density of an appropriately paramaterized exponential distribution with the histogram of time spent alone, you can see a decent fit. So based on our investigation both in the last several lectures and in last weeks lab, $N =200$ should be more than enough for the CLT approximation to be a good one.

## Question 3: Using the CLT, and assuming for a moment that the population standard deviation is $150$ minutes, and assuming a sample of size $N=200$, what will the length of a $90\%$ confidence interval for the population mean time spent alone be?

Usually we don't have access to the population standard deviation, so we estimate it using the sample data.

As we've discussed, the law of large numbers provides justification for using the sample standard deviation $\hat{\sigma}_n$ to estimate the population standard deviation.

Also, the [algebra of convergence in distribution](https://en.wikipedia.org/wiki/Slutsky%27s_theorem) justifies replacing $\sigma$ with $\hat{\sigma}_n$ in statements about convergence in distribution.

So it is valid to use the sample standard deviation ($\hat{\sigma}_n$) in place of the population standard deviation when applying the CLT. Specifically, instead of using the approximation
\[
\bar{X}_{200} \overset{d}{\approx} N(\mu,\sigma^2/200)
\]
we can use the approximation
\[
\bar{X}_{200} \overset{d}{\approx} N(\mu,\hat{\sigma}_{200}^2/200)
\]

But in class on Tuesday we learned the exact distribution of $\frac{\sqrt{n}}{{\hat{\sigma}_{200}}} (\bar{X}_{200}  - \mu)$. **But only when the population is normally distributed**

If you know the exact distribution of a statistic use it (rather than an approximation like the CLT) to construct the confidence interval. 

Side note: As the degrees of freedom of the $t$ distribution tends to infinity, the $t$ distribution converges to the standard normal. i.e 
\[t_{df} \overset{d}{\to} N(0,1) \text{ as }df \to \infty\]

If we had erroneously assumed time spent alone was normally distributed, we would have concluded that
\[
\frac{\sqrt{200}}{\hat{\sigma_n}}(\bar{X}_{200}-\mu) \sim t_{199}
\]
Instead we used the CLT and concluded 
\[
  \frac{\sqrt{n}}{{\hat{\sigma}_{200}}} (\bar{X}_{200}  - \mu) \overset{\approx}{\sim} N(0,1)
\]

But because $t_{df} \to N(0,1)$ as $df \to \infty$ and $df = 199$ is quite large, there is very little difference between $t_{199}$ and $N(0,1)$.

Since both of these distribution are symmetric, in $90\%$ confidence interval construction, we are interested in the $95^{th}$ quantile of whatever distribution we believe our statistic follows. The $95^{th}$ quantile of the $t_{199}$ is $1.652547$ while the $95^{th}$ quantile of $N(0,1)$ is $1.644854$. Not much difference.

In other words, when $N$ is large, both of these approaches will lead to very similar inferences. The probability that a $t$ based interval will cover the mean is slightly higher than that of the $N(0,1)$ (CLT) based approach since the $t$ distribution has heavier tails than the $N(0,1)$.


*Now assume we independently collect 200 samples from each of the 10 ATUS variables and then construct exact $90\%$ confidence intervals for the mean for each variable.*

Let $Y =$ Number of Intervals Not Covering the Mean

## Question 4: What is the distribution of $Y$?

## Question 5: What is $P(Y \geq 1)$?. Hint: $1 - P(Y = 0)$. Note: Even if you aren't sure about Question 4, you can still answer this question using properties of independent events



Here is some code that does this procedure using our CLT based approach -- which is fine since is $N$ is large. You'll fill in a couple of details to make the code work. Then you'll construct the 10 intervals using the filled in code.

```{r,eval = FALSE}
#Read in the data
dataSubset1 <- read.csv("data/populationData/nonNAdata.csv",header = TRUE)
m <- nrow(dataSubset1)
n <- 200
sampledOne <- dataSubset1[sample(m,size = n,replace = TRUE),]
trernhlyS <- sample(read.csv("data/populationData/TRERNHLYadjusted.csv")[,2],size = n,replace = TRUE)
trernwaS <- sample(read.csv("data/populationData/TRERNWAadjusted.csv")[,2],size = n,replace = TRUE)
tucc2S <- sample(read.csv("data/populationData/TUCC2adjusted.csv")[,2],size = n,
                 replace = TRUE)
tucc4S <- sample(read.csv("data/populationData/TUCC4adjusted.csv")[,2],size = n,
                 replace = TRUE)
sampledData <- cbind(sampledOne,trernhlyS,trernwaS,tucc2S,tucc4S)[,-1]

#get the sample means
mns <- apply(X = sampledData, MARGIN = 2, FUN = mean)
#get the sample sds
sds <- apply(X = sampledData, MARGIN = 2, FUN = sd)

#Get the quantile needed for CI construction
qntl <- ? [see question 7]

#construct the confidence intervals 
intervalLow <- mns - qntl*(sds/sqrt(n))
intervalHigh <- mns+qntl*(sds/sqrt(n))

#These are the population means.
trueMeans <- c(22.228,114.421,3.964,362.0627,295.0263,.6522,21.3226,1176.6790,451.7254,1212.3868)

frame <- data.frame(intervalLow,trueMeans,intervalHigh)

numFailedIntervals <- sum(trueMeans < intervalLow | trueMeans > intervalHigh)
```

## Question 6: What code should be filled in on the line where `qntl` is defined in order to construct the $90\%$ (CLT based) confidence intervals?

Now based on your answer to question $6$, fill in the line for `qntl` and run the code.

## Question 7: Report the number of intervals that failed to cover the mean.

## Question 8: If instead we decided to make intervals based on the $t$ distribution -- still $90\%$ -- what code should be filled in on the line where `qntl` is defined?



