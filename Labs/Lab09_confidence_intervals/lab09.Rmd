---
title: 'Lab 9: Confidence Intervals For the Mean Via the CLT'
output: html_document
---

We'll be analyzing how Americans spend their time.

The [American Time Use Survey](https://www.bls.gov/tus/overview.htm#1) is an annual survey that provides nationally representative data about the amount of time Americans spend doing various activities, such as commuting to work, watching television, or caring for children, and provides ancillary variables such as demographic characteristics and other factors.  These data a frequently used by researchers, journalists, educators, sociologists, economists, government lawmakers, lawyers, city planners and individuals such as yourself.

The 2020 survey interviewed $8,782$ people.  Some variables collected include:
 
 <ul>
  <li>TRTFRIEND: Total nonwork-related time respondent spent with friends (in minutes) [Min:0, Max: 1440] </li>
  <li>TRTSPONLY: Total nonwork-related time respondent spent with spouse only (in minutes) </li>
  <li>TRTCCC: Total nonwork-related time respondent spent with customers, clients, and coworkers (in minutes)</li>
  <li>TRTALONE: Total nonwork-related time respondent spent alone (in minutes) </li>
  <li>TRTFAMILY: Total nonwork-related time respondent spent with family members 
 </li>
 <li>TRCHILDNUM: Number of household Children < 18 </li>
 <li> TRERNWA: Weekly earnings at main job [For non-hourly wage earners]</li>
 <li> TRERNHLY: Hourly earnings at main job [For hourly wage earners]</li>
 <li> TUCC2: Time first child < 13 woke up [For those with children -- in number of minutes past midnight]</li>
 <li> TUCC4: Time last household child < 13 went to bed [For those with children] -- in number of minutes past midnight</li>
</ul>

The population of interest is Americans and how they spend their time on a typical weekday. So there are $10$ distributions of interest, and we would like to do inference for the mean of each of these distributions.

Although contrived, from now on we will assume that the entire ATUS survey results **are the entire** population -- i.e we are assuming that every American is included in the ATUS survey results.  The point of this is so that we can investigate the relationship between "confidence", a sample, and the population.  In practice, of course, if one were attempting to make inferences about the time use of Americans, one would simply use the entire ATUS dataset. 

i.e will use a random subsampling of the ATUS survey results to do inference about the population of all Americans surveyed by ATUS.

You can find most of this "population" data in `data/populationData/nonNAdata.csv`. After loading in this dataset, answer question 1.

## Question 1: Use the population dataset to compute the mean total time spent alone (in minutes) on a week day for Americans in 2020

Using an approach like that you used in question 1, here are our population means for the other variables.

*  TRTFRIEND: 22.228 minutes 
*  TRTSPONLY: 114.421 minutes
*  TRTCCC: 3.964 minutes
*  TRTFAMILY: 295.0263 minutes
*  TRCHILDNUM: .6522 children 
*  TRERNWA: 21.3226\$
*  TRERNHLY: 1176.6790\$
*  TUCC2: 451.7254 minutes
*  TUCC4: 1212.3868 minutes

## Question 2: What is the population standard deviation for time spent alone?

Now lets imagine we didn't have access to the population. We only have access to a sample of 200 independent and identically distribution observations for each variable.

Let $\bar{X}_{200}$ be the sample mean time spent alone. Because our samples are independent and identically distributed from the population, the CLT gives 
\[
\frac{\sqrt{n}}{\sigma} (\bar{X}_n - \mu_{alone}) \overset{d}{\to} N(0,1)
\]
So for sufficiently large $n$, the approximation
$ \frac{\sqrt{n}}{\sigma} (\bar{X}_n - \mu) \overset{d}{\approx} N(0,1)$ is reasonable.

There are a couple of details we are glossing over here. One is that we don't necessarily know that the variance of the distribution of time spent alone even exists. And even if it did, we learned last week that the [skewness of a distribution determines the speed of convergence in the CLT](https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem).

But for now, lets assume that the variance of the distribution of time spent alone does exist, and higher moments of this distribution behave in such a way that
$\bar{X}_{200} \overset{d}{\approx} N(\mu,\sigma^2/200)$ is not a bad approximation.

Side note: If you compare the probability density of an appropriately paramaterized exponential distribution with the histogram of time spent alone, you can see a decent fit. So based on our investigation both in the last several lectures and in last weeks lab, $N =200$ should be more than enough for the CLT approximation to be a good one.

## Question 3: Using the CLT, and the population standard deviation you computed in question 2, find a $90\%$ confidence interval for the population mean time spent alone by using the sample data.

Usually we don't have access to the population standard deviation, so we estimate it using the sample data.

As we've discussed, the law of large numbers provides justification for using the sample standard deviation $\hat{\sigma}_n$ to estimate the population standard deviation.

Also, the [algebra of convergence in distribution](https://en.wikipedia.org/wiki/Slutsky%27s_theorem) justifies replacing $\sigma^2$ with $\hat{\sigma}_n^2$ in statements about convergence in distribution.

So it is valid to use the sample standard deviation ($\hat{\sigma_n}$) in place of the population standard deviation when applying the CLT. Specifically, instead of using the approximation
\[
\bar{X}_{200} \overset{d}{\approx} N(\mu,\sigma^2/200)
\]
we can use the approximation
\[
\frac{\sqrt{n}}{{\hat{\sigma}_{200}}} (\bar{X}_{200}  - \mu) \overset{d}{\approx} N(0,1)
\]

## Question 4: Using the CLT, and the sample standard deviation for time spent alone, compute a $90\%$ confidence interval for the population mean time spent alone

We just approximated the distribution of 
\[
\frac{\sqrt{n}}{{\hat{\sigma}_{200}}} (\bar{X}_{200}  - \mu)
\]
to construct a confidence interval. 

But you might be scratching your head as to why we did that. In class on Tuesday, we learned the exact distribution of $\frac{\sqrt{n}}{{\hat{\sigma}_{200}}} (\bar{X}_{200}  - \mu)$. **But only when the population is normally distributed**

If you know the exact distribution of a statistic use it (rather than an approximation like the CLT) to construct the confidence interval. 

T-TEST stuff: If we're going to add it, this might be a good place



## Question 5: Assuming we independently collected 200 samples from each of the 10 ATUS variables and then construct exact $90\%$ confidence intervals for the mean for each variable, what is the probability that at least one interval doesn't cover the mean?

Let's say that $1000$ different researchers in isolated environments  independently collect samples of size $200$ from the distribution of time spent alone by Americans. Then they construct **exact** $90\%$ confidence intervals for the mean time spent alone.

We can describe this procedure visually, where $t_{1,1}$ is the time spent alone collected from the first individual by the first researcher, $t_{1,2}$ is the time spent alone collected from the second individual by the first researcher, and so on. $I_1$ is the confidence interval for the mean constructed by the first researcher, $I_2$ is the confidence interval for the mean constructed by the second researcher and so on.

So each row is the data collected by a researcher, and the $j^{th}$ researchers interval is $I_{j}$
\[
\begin{equation}
  \left(
    \begin{array}{*5{c}}
    t_{1,1} & t_{1,2} & t_{1,3} & \dots & t_{1,200} & \rightarrow & I_1 \\
    t_{2,1} & t_{2,2} & t_{2,3} & \dots & t_{2,200} & \rightarrow & I_2\\
     \vdots & \vdots & \vdots & \vdots  & \vdots & \rightarrow & \vdots\\
    t_{1000,1} & t_{1000,2} & t_{1000,3} & \dots & t_{1000,200} & \rightarrow & I_{2000}
  \end{array}\right)
\end{equation}
\]

Let $Y$ be the number of researchers whose intervals don't cover $\mu$.

## Question 6: What is the distribution of $Y$?

## Question 7: What is P(Y > 100)?

Here is some code to simulate the data collection procedure of the $1000$ researchers:

```{r,eval = FALSE}
library(tidyverse)
##Read in the data
timeSpentAlonePopulation = read.csv("data/populationData/nonNAdata.csv",header = TRUE) %>%
  select(TRTALONE) %>% unlist()
sampleMatrix = matrix(sample(timeSpentAlonePopulation,size = 200*1000,
                             replace = TRUE),nrow = 1000,ncol = 200)

```
And here is some follow up code that would compute the $1000$ intervals assumign the CLT provides an accurate approximation:

```{r,eval = FALSE}
trueMean = 362.0627
n = 200
z = "What you will fill in for question 8"
mns <- apply(X = sampleMatrix, MARGIN = 1, FUN = mean)
sds <- apply(X = sampleMatrix, MARGIN = 1, FUN = sd)
intervalLow <- mns - z*(sds/sqrt(n))
intervalHigh <- mns+z*(sds/sqrt(n))
numFailedIntervals <- sum(trueMean < intervalLow | trueMean > intervalHigh)
```

## Question 8: What code gives $z$? Recall we are assuming the CLT approximation is valid and we are constructing $90\%$ confidence intervals.

## Question 9: After you fill in the code for $z$, run the code to find the number of researchers whose intervals have failed to cover the true mean in the population. Do we conclude these researchers did something wrong?





